---
title: "Reading Data From the Web"
author: "Caroline Andy"
date: "10/18/2020"
output: html_document
---

In this section we will learn how to gather data from online sources (i.e. "scrape) using APIs, rvest and httr.

```{r packages}
library(tidyverse)
library(rvest)
library(httr)
```

### Extracting tables

We will pull data on drug use in the past year or month from the National Survey on Drug Use and Health.

First, let's make sure we can load data from the web:
```{r load}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_html = read_html(url)

drug_use_html
```

Let's try extracting the tables from the HTML:
```{r extract}
drug_use_html %>%
  html_nodes(css = "table")
```

This contains all 15 tables included in the URL. We want to focus on only the first one. 

```{r table}
table_marj =
  drug_use_html %>%
  html_nodes(css = "table") %>%
  first() %>%
  html_table()
```

We can see that the note at the bottom of the table appears in every column in the first row. We need to remove this. We are also converting to a tibble so that things print nicely.

```{r tibble}
table_marj =
  drug_use_html %>%
  html_nodes(css = "table") %>%
  first() %>%
  html_table() %>%
  slice(-1) %>%
  as_tibble()

table_marj
```

Learning assessment: create a data frame that contains the cost of living table for New York from a provided webpage.

```{r learning_assessment}
nyc_cost = 
  read_html("https://www.bestplaces.net/cost_of_living/city/new_york/new_york") %>%
  html_nodes(css = "table") %>%
  .[[1]] %>%
  html_table(header = TRUE)
```

###CSS Selectors

Suppose we'd like to scrape the data about the Star Wars Movies from the IMDB page. The first step is the same as before - we need to get the HTML.

```{r CSS_Select}
swm_html = 
  read_html("https://www.imdb.com/list/ls070150896/")
```

The data we are interested in is not stored in a handy table, so we're going to isolate the CSS selector for elements we care about. 

```{r CSS_Select2}
title_vec = 
  swm_html %>%
  html_nodes(".lister-item-header a") %>%
  html_text()

gross_rev_vec = 
  swm_html %>%
  html_nodes(".text-small:nth-child(7) span:nth-child(5)") %>%
  html_text()

runtime_vec = 
  swm_html %>%
  html_nodes(".runtime") %>%
  html_text()

swm_df = 
  tibble(
    title = title_vec,
    rev = gross_rev_vec,
    runtime = runtime_vec)
```

Learning assessment: we will look at a page containing the 10 most recent reviews of the movie "Napoleon Dynamite." Use a process similar to the one above to extract the titles of the reviews.

```{r learning_assessment2}
url = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber=1"

dynamite_html = read_html(url)

review_titles = 
  dynamite_html %>%
  html_nodes(".a-text-bold span") %>%
  html_text()

review_stars = 
  dynamite_html %>%
  html_nodes(".review-rating") %>%
  html_text()

review_text = 
  dynamite_html %>%
  html_nodes(".review-text-content span") %>%
  html_text()

reviews = tibble(
  title = review_titles,
  stars = review_stars,
  text = review_text
)
```

### Using an API

APIs = Application Programming Interfaces 

Now we will look at a dataset containing annual water consumption in NYC by population. First we will import this as a CSV and parse it. 

```{r import}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content("parsed")
```

Data.gov also has a lot of data available using their API; often this is available as CSV of JSON files. We might be interested in data coming from BRFSS. This is importable via the API as a CSV.

```{r API}
brfss_smart2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv",
      query = list("$limit" = 5000)) %>% 
  content("parsed")
```

By default, the CDC API limits data to the first 1000 rows. Here I've increased that by changing an element of the API query - I looked around the website describing the API to find the name of the argument, and then used the appropriate syntax for GET. To get the full data, I could increase this so that I get all the data at once or I could try iterating over chunks of a few thousand rows. 

To get a sense of how this becomes complicated, let's look at the Pokemon API

```{r pokemon}
poke =   
  GET("http://pokeapi.co/api/v2/pokemon/1") %>%
  content()

poke$name
poke$height
poke$abilities
```

To build a Pokemon dataset for analysis, you’d need to distill the data returned from the API into a useful format; iterate across all pokemon; and combine the results.

For both of the API examples we saw today, it wouldn’t be terrible to just download the CSV, document where it came from carefully, and move on. APIs are more helpful when the full dataset is complex and you only need pieces, or when the data are updated regularly.



